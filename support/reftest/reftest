#!/usr/bin/env python3
"""

Prepare reference test data for Exodus-lambda integration test.
It requires a "data.yml" file specifying the reference test data.

"""

import argparse
import hashlib
import json
import os
import pprint
import sys
import tempfile
import time
from datetime import datetime, timezone
from typing import Optional

import boto3
import requests
import yaml
from botocore.config import Config
from botocore.exceptions import ClientError
from tqdm import tqdm

DATA_DIR = os.path.dirname(__file__)

ENDPOINT_URL = os.environ.get("EXODUS_AWS_ENDPOINT_URL") or None


def die(message: str):
    print(message, file=sys.stderr)
    sys.exit(4)


def check_response(response: requests.Response):
    # like response.raise_for_status(), but ensures response body is output.
    if response.status_code >= 200 and response.status_code < 400:
        return

    response_body = response.content

    try:
        # if it's JSON, try to get it pretty-printed instead.
        response_body = pprint.pformat(response.json(), indent=2)
    except:
        # whatever, guess it wasn't JSON then...
        pass

    die(
        f"{response.status_code} {response.reason} for {response.url}\n\n"
        + response_body
    )


class DBHandler:
    def __init__(self, table, config_table, session):
        self.dynamodb = session.client("dynamodb", endpoint_url=ENDPOINT_URL)
        self.table = table
        self.config_table = config_table

        self.default_from_date = datetime.utcnow().isoformat(
            timespec="seconds"
        )

    def put_item(
        self,
        web_uri,
        object_key,
        from_date=None,
        content_type=None,
        metadata={},
    ):
        if not from_date:
            from_date = self.default_from_date

        item = {
            "web_uri": {"S": web_uri},
            "from_date": {"S": from_date},
            "object_key": {"S": object_key},
            "metadata": {"M": metadata},
        }

        if content_type:
            item["content_type"] = {"S": content_type}

        self.dynamodb.put_item(
            TableName=self.table,
            Item=item,
        )

    def put_config(self, config, from_date=None):
        if not from_date:
            from_date = self.default_from_date

        item = {
            "from_date": {"S": from_date},
            "config_id": {"S": "exodus-config"},
            "config": {"S": json.dumps(config)},
        }

        self.dynamodb.put_item(
            TableName=self.config_table,
            Item=item,
        )

    def put_absent_item(self, path, from_date=None):
        if not from_date:
            from_date = self.default_from_date

        item = {
            "web_uri": {"S": path},
            "from_date": {"S": from_date},
            "object_key": {"S": "absent"},
            "content_type": {"S": ""},
            "metadata": {"M": {}},
        }

        self.dynamodb.put_item(
            TableName=self.table,
            Item=item,
        )


class S3Handler:
    def __init__(self, bucket, session):
        self.s3_client = session.client("s3", endpoint_url=ENDPOINT_URL)
        self.bucket = bucket

    def upload_from_localfile(self, path, checksum):
        st = os.stat(path)
        bar = tqdm(
            desc="%s [upload]" % checksum,
            total=st.st_size,
            unit="iB",
            unit_scale=True,
        )

        try:
            with open(path, "rb") as data:
                self.s3_client.upload_fileobj(
                    Fileobj=data,
                    Bucket=self.bucket,
                    Key=checksum,
                    Callback=bar.update,
                )
        finally:
            bar.close()


class PublishBackend:
    """Base class for different publish backends."""

    def start_publish(self):
        """Start a publish.

        May do nothing if backend does not have the concept of grouping
        items into a publish object.
        """

    def commit(self):
        """Commit a publish previously started via start_publish.

        May do nothing if backend does not have the concept of grouping
        items into a publish object.
        """

    def set_exodus_config(self, config: dict):
        """Set the current exodus-config to the given dict."""

        # subclass must implement
        raise NotImplementedError()

    def add_item(
        self,
        local_path: str,
        remote_path: str,
        sha256sum: str,
        content_type: Optional[str],
    ):
        """Add a single item onto the current publish:

        local_path   -- path to a local file
        remote_path  -- path used for published content on CDN
        sha256sum    -- checksum of the content
        content_type -- MIME type of uploaded content
        """
        # subclass must implement
        raise NotImplementedError()


class AWSPublishBackend(PublishBackend):
    """Publish backend using low-level access to AWS resources."""

    def __init__(self, opts: argparse.Namespace):
        session = boto3.Session(
            aws_access_key_id=opts.aws_access_id,
            aws_secret_access_key=opts.aws_access_key,
            aws_session_token=opts.aws_session_token,
            region_name=opts.default_region,
        )

        if not opts.table:
            die("Missing --table argument for AWS.")
        if not opts.bucket:
            die("Missing --bucket argument for AWS.")
        if not opts.config_table:
            die("Missing --config-table for AWS.")

        self.db = DBHandler(
            table=opts.table, config_table=opts.config_table, session=session
        )
        self.s3 = S3Handler(bucket=opts.bucket, session=session)

    def add_item(
        self,
        local_path: str,
        remote_path: str,
        sha256sum: str,
        content_type: Optional[str],
    ):
        # push test data to s3 and dynamodb
        self.s3.upload_from_localfile(local_path, sha256sum)
        self.db.put_item(
            remote_path,
            sha256sum,
            from_date=str(datetime.now(timezone.utc)),
            content_type=content_type,
        )

    def set_exodus_config(self, config: dict):
        self.db.put_config(config)


class GWPublishBackend(PublishBackend):
    """Publish backend using exodus-gw."""

    def __init__(self, opts: argparse.Namespace):
        self.session = requests.Session()
        self.session.cert = (opts.gw_cert, opts.gw_key)

        if not opts.gw_env:
            die("Missing --gw-env.")
        if not opts.gw_url:
            die("Missing --gw-url.")

        self.env = opts.gw_env
        self.url = opts.gw_url
        while self.url.endswith("/"):
            self.url = self.url[:-1]

        # https://release-engineering.github.io/exodus-gw/api.html#section/Using-boto3-with-the-upload-API
        s3 = boto3.resource(
            "s3",
            endpoint_url=f"{self.url}/upload",
            aws_access_key_id="dummy",
            aws_secret_access_key="dummy",
            config=Config(client_cert=(opts.gw_cert, opts.gw_key)),
        )
        self.s3_bucket = s3.Bucket(opts.gw_env)

        self.publish_url = None
        self.commit_url = None

    def start_publish(self):
        create_url = f"{self.url}/{self.env}/publish"

        response = self.session.post(create_url)
        check_response(response)

        body = response.json()
        self.publish_url = self.url + body["links"]["self"]
        self.commit_url = self.url + body["links"]["commit"]

        print("Started publish:", self.publish_url)

    def commit(self):
        print("Committing:", self.publish_url)

        response = self.session.post(self.commit_url)
        check_response(response)

        body = response.json()
        task_url = self.url + body["links"]["self"]
        self.await_task(task_url)

    def await_task(self, task_url: str):
        print("Awaiting task:", task_url)

        while True:
            response = self.session.get(task_url)
            check_response(response)

            state = response.json()["state"]
            print("   ", state)
            if state == "FAILED":
                die(f"exodus-gw task failed: {task_url}")
            if state == "COMPLETE":
                break
            time.sleep(10)

    def add_item(
        self,
        local_path: str,
        remote_path: str,
        sha256sum: str,
        content_type: Optional[str],
    ):
        # First ensure the object is in S3.
        object = self.s3_bucket.Object(sha256sum)

        try:
            object.load()
            print("Already in S3:", sha256sum)
        except ClientError as exc:
            if exc.response["Error"]["Code"] == "404":
                # The normal case for a missing object, so upload it.
                object.upload_file(local_path)
                print(f"Uploaded: {sha256sum}")
            else:
                # Anything else, we don't know what happened, propagate.
                raise

        # Next add it to the publish.
        item = dict(
            web_uri=remote_path,
            object_key=sha256sum,
        )
        if content_type:
            item["content_type"] = content_type
        response = self.session.put(self.publish_url, json=[item])
        check_response(response)

        print(f"Added to publish: {remote_path} => {sha256sum}")

    def set_exodus_config(self, config: dict):
        deploy_config_url = f"{self.url}/{self.env}/deploy-config"

        print("Deploying config:", config)
        response = self.session.post(deploy_config_url, json=config)
        check_response(response)

        task_url = self.url + response.json()["links"]["self"]
        self.await_task(task_url)


def get_publish_backend(opts: argparse.Namespace) -> PublishBackend:
    """Obtain and return a PublishBackend appropriate for the arguments
    passed by the user.
    """
    if opts.bucket or opts.table or opts.config_table:
        # AWS mode.
        if opts.gw_url or opts.gw_env:
            die(
                "You provided both AWS and exodus-gw arguments. "
                "Provide only one or the other."
            )

        return AWSPublishBackend(opts)

    if opts.gw_url or opts.gw_env:
        # GW mode.
        if opts.bucket or opts.table or opts.config_table:
            die(
                "You provided both AWS and exodus-gw arguments. "
                "Provide only one or the other."
            )
        return GWPublishBackend(opts)

    # broken mode
    die(
        "You must provide either AWS arguments (--bucket, ...) or "
        "exodus-gw arguments (--gw-url, ...). See --help."
    )


def parse_aws_session(parser):
    parser.add_argument(
        "--aws-access-id",
        default=None,
        help="Access ID for Amazon services. If no ID is provided, attempts to"
        " find it among environment variables and ~/.aws/config file will"
        " be made",
    )
    parser.add_argument(
        "--aws-access-key",
        default=None,
        help="Access key for Amazon services. If no key is provided, attempts"
        " to find it among environment variables and ~/.aws/config file"
        " will be made",
    )
    parser.add_argument(
        "--aws-session-token",
        default=None,
        help="Session token for Amazon services. If no token is provided,"
        " attempts to find it among environment variables and"
        " ~/.aws/config file will be made",
    )
    parser.add_argument(
        "--default-region",
        default="us-east-1",
        help="Default region for Amamzon services. If no region is provided,"
        " it will go with us-east-1",
    )


def parse_cdn_certs(parser):
    parser.add_argument(
        "--cert",
        default=os.path.expanduser("~/certs/rcm-debug/rcm-debug.crt"),
        type=str,
        help="Client certificate file and password",
    )
    parser.add_argument(
        "--key",
        default=os.path.expanduser("~/certs/rcm-debug/rcm-debug.key"),
        type=str,
        help="Private key file name",
    )
    parser.add_argument(
        "--cacert",
        default="/etc/rhsm/ca/redhat-uep.pem",
        type=str,
        help="CA certificate to verify peer against",
    )


def parse_args():
    root_parser = argparse.ArgumentParser(description=__doc__)
    subparsers = root_parser.add_subparsers(
        dest="command", help="reference test data operations", required=True
    )
    # parser for prepare operation
    parser_prepare = subparsers.add_parser(
        "prepare",
        help="""
        prepare reference test data in dynamodb and s3
        for integration test,
         e.g. $./reftest prepare --bucket exodus-bucket --table exodus-table --config-table exodus-config
        """,
    )

    parser_prepare.add_argument(
        "--deploy-config",
        action="store_true",
        help="Also deploy CDN config (dangerous!)",
    )

    parse_cdn_certs(parser_prepare)

    aws_group = parser_prepare.add_argument_group("AWS arguments")
    parse_aws_session(aws_group)
    aws_group.add_argument(
        "--bucket",
        help="The AWS S3 bucket used to store test data",
    )
    aws_group.add_argument(
        "--table",
        help="The AWS dynamoDB used to store test data",
    )
    aws_group.add_argument(
        "--config-table",
        help="The AWS dynamoDB used to store Exodus config",
    )

    gw_group = parser_prepare.add_argument_group("exodus-gw arguments")
    gw_group.add_argument(
        "--gw-url",
        help="Base URL of an exodus-gw instance",
    )
    gw_group.add_argument(
        "--gw-env",
        help="Environment name (e.g. pre, live)",
    )
    gw_group.add_argument(
        "--gw-cert",
        help="Path to a cert for exodus-gw auth",
        default=os.path.expandvars("$HOME/certs/$USER.crt"),
    )
    gw_group.add_argument(
        "--gw-key",
        help="Path to the key corresponding with --gw-cert",
        default=os.path.expandvars("$HOME/certs/$USER.key"),
    )

    return root_parser.parse_args()


class RefTestConfig:
    def __init__(self, prod_cdn_url, test_data, test_config):
        self.prod_cdn_url = prod_cdn_url
        self.test_data = test_data
        self.test_config = test_config


def load_config():
    with open(os.path.join(DATA_DIR, "data.yml")) as data_file:
        data = yaml.load(data_file, yaml.SafeLoader)

    with open(os.path.join(DATA_DIR, "exodus-config.json")) as config_file:
        config = json.load(config_file)

    return RefTestConfig(data["prod-cdn-url"], data["test_data"], config)


# It will return a TempFileObj and a checksum for test data verification
def download_to_local(url, key_path, cert_path, cacert_path):
    with requests.get(
        url,
        cert=(cert_path, key_path),
        verify=cacert_path,
        stream=True,
        timeout=(30, 30),
    ) as req:
        check_response(req)
        total_size = int(req.headers.get("content-length", 0))
        tbar = tqdm(desc=url, total=total_size, unit="iB", unit_scale=True)

        # the file is deleted as soon as it is closed.
        temp_file = tempfile.NamedTemporaryFile(delete=True)

        sha256 = hashlib.sha256()

        for chunk in req.iter_content(chunk_size=8192):
            if chunk:  # filter out keep-alive new chunks
                tbar.update(len(chunk))
                temp_file.write(chunk)
                sha256.update(chunk)
                temp_file.flush()
        tbar.close()
        return temp_file, sha256.hexdigest()


def prepare(publisher: PublishBackend, config, opt):
    publisher.start_publish()
    for item in config.test_data:
        if not item.get("deploy", True):
            # skip items that specify deploy=false
            continue

        if item.get("state") == "absent":
            # Just don't put any item.
            continue

        url = config.prod_cdn_url + item["path"]
        # download test data to local with a name "NamedTemporaryFile"
        temp_file, cdn_data_checksum = download_to_local(
            url, opt.key, opt.cert, opt.cacert
        )

        # For unstable content which did not provide sha256, it will skip the
        # checksum verify
        if item.get("sha256") and cdn_data_checksum != item["sha256"]:
            print(
                "{} verify checksum failed, cdn_data_checksum is {}, ".format(
                    item["path"], cdn_data_checksum
                )
                + "but test_data_checksum is {}".format(item["sha256"])
            )
            return False

        publisher.add_item(
            local_path=temp_file.name,
            remote_path=item["path"],
            sha256sum=cdn_data_checksum,
            content_type=item.get("content-type"),
        )

        # delete the NamedTemporaryFile
        temp_file.close()

    publisher.commit()

    if opt.deploy_config:
        # deploy Exodus config to config table
        publisher.set_exodus_config(config.test_config)

    return True


def main():
    config = load_config()
    opt = parse_args()
    publisher = get_publish_backend(opt)

    res = False
    if opt.command == "prepare":
        res = prepare(publisher, config, opt)

    if res:
        print("{} operation has finished successfully!".format(opt.command))
    else:
        print("Fatal error: {} operation is terminated".format(opt.command))
        return 1


if __name__ == "__main__":
    sys.exit(main())
